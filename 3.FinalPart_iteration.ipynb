{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc12aec",
   "metadata": {},
   "source": [
    "Author: Anirudh Iyer\n",
    "Contact Information: aniiyer@iu.edu\n",
    "Date: 9/19/2024\n",
    "\n",
    "Topic: Continual learning through incremental data pipeline on CIFAR-10 using pretrained inceptionv3\n",
    "    \n",
    "Description of the Work:\n",
    " \n",
    "    Data Preparation:\n",
    "\n",
    "    The dataset used is CIFAR-10, and it is divided into smaller subsets with 2 classes each.\n",
    "    train_subsets and test_subsets are created for continual learning, splitting the data in increments.\n",
    "    \n",
    "    Training Function (train_increment):\n",
    "\n",
    "    This function trains the model in increments using the provided data loader.\n",
    "    It calculates the loss using a custom Magnet Loss function, optimizes the model, and prints batch losses.\n",
    "\n",
    "    Visualization and Reporting:\n",
    "\n",
    "    After each training increment, plots of the progress and embeddings are created.\n",
    "    A classification report and confusion matrix are printed for the performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7099791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models \n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset, ConcatDataset\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96af17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdaptiveMagnetLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(AdaptiveMagnetLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, embeddings, labels, m, d):\n",
    "        \"\"\"\n",
    "        Compute the Magnet Loss.\n",
    "        \n",
    "        Args:\n",
    "        embeddings: Tensor of shape (batch_size, embedding_dim)\n",
    "        labels: Tensor of shape (batch_size,)\n",
    "        m: Number of clusters per class (will be adjusted based on batch size)\n",
    "        d: Minimum examples per cluster (will be adjusted based on batch size)\n",
    "        \n",
    "        Returns:\n",
    "        loss: Scalar tensor with the computed loss\n",
    "        \"\"\"\n",
    "        device = embeddings.device\n",
    "        batch_size, embedding_dim = embeddings.size()\n",
    "        \n",
    "        # Adjust m and d based on the actual batch size\n",
    "        adjusted_m = min(m, batch_size // 2)  # Ensure at least 2 samples per cluster\n",
    "        adjusted_d = max(2, batch_size // adjusted_m)  # Ensure at least 2 samples per cluster\n",
    "        \n",
    "        # Create cluster assignments\n",
    "        unique_labels = torch.unique(labels)\n",
    "        num_classes = len(unique_labels)\n",
    "        \n",
    "        cluster_means = []\n",
    "        cluster_labels = []\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            label_mask = labels == label\n",
    "            label_embeddings = embeddings[label_mask]\n",
    "            \n",
    "            # Adjust number of clusters for this class\n",
    "            class_size = label_embeddings.size(0)\n",
    "            class_m = min(adjusted_m, class_size)\n",
    "            \n",
    "            # Simple clustering: divide samples evenly among clusters\n",
    "            class_clusters = torch.arange(class_size) % class_m\n",
    "            \n",
    "            for i in range(class_m):\n",
    "                cluster_mask = class_clusters == i\n",
    "                if torch.sum(cluster_mask) > 0:\n",
    "                    cluster_means.append(torch.mean(label_embeddings[cluster_mask], dim=0))\n",
    "                    cluster_labels.append(label)\n",
    "        \n",
    "        cluster_means = torch.stack(cluster_means)\n",
    "        cluster_labels = torch.tensor(cluster_labels, device=device)\n",
    "        \n",
    "        # Compute variance (use a small epsilon to avoid division by zero)\n",
    "        variance = torch.sum((embeddings - embeddings.mean(dim=0))**2) / (batch_size - 1)\n",
    "        var_normalizer = -1 / (2 * variance.clamp(min=1e-10))\n",
    "        \n",
    "        # Compute distances\n",
    "        distances = torch.cdist(embeddings, cluster_means)**2\n",
    "        \n",
    "        # Compute intra-cluster and inter-cluster costs\n",
    "        same_label_mask = labels.unsqueeze(1) == cluster_labels.unsqueeze(0)\n",
    "        intra_cluster_costs = torch.where(same_label_mask, distances, torch.tensor(float('inf'), device=device))\n",
    "        inter_cluster_costs = torch.where(same_label_mask, torch.tensor(float('inf'), device=device), distances)\n",
    "        \n",
    "        # Compute loss\n",
    "        intra_cluster_costs, _ = intra_cluster_costs.min(dim=1)\n",
    "        inter_cluster_costs, _ = inter_cluster_costs.min(dim=1)\n",
    "        \n",
    "        loss = F.relu(intra_cluster_costs - inter_cluster_costs + self.alpha)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acdacd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagnetLoss(nn.Module): # Not used\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(MagnetLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, r, classes, m, d):\n",
    "        device = r.device\n",
    "        classes = classes.to(device)\n",
    "        \n",
    "        # Adjust m and d based on the actual batch size\n",
    "        batch_size = r.size(0)\n",
    "        adjusted_m = max(2, min(m, batch_size // d))\n",
    "        adjusted_d = batch_size // adjusted_m\n",
    "\n",
    "        clusters = torch.sort(torch.arange(0, float(adjusted_m)).repeat(adjusted_d))[0].to(device)\n",
    "        cluster_classes = classes[0:adjusted_m*adjusted_d:adjusted_d]\n",
    "\n",
    "        # Compute variance\n",
    "        variance = torch.sum((r - torch.mean(r, dim=0))**2) / (r.size(0) - 1)\n",
    "        var_normalizer = -1 / (2 * variance**2)\n",
    "\n",
    "        # Compute cluster means\n",
    "        cluster_means = torch.stack([torch.mean(r[clusters == i], dim=0) for i in range(adjusted_m)])\n",
    "\n",
    "        # Compute distances\n",
    "        sample_costs = torch.cdist(cluster_means, r.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # Compute intra-cluster costs\n",
    "        intra_cluster_mask = (clusters.unsqueeze(1) == torch.arange(adjusted_m, device=device).unsqueeze(0)).float()\n",
    "        intra_cluster_costs = torch.sum(intra_cluster_mask * sample_costs, dim=1)\n",
    "\n",
    "        # Compute numerator and denominator\n",
    "        numerator = torch.exp(var_normalizer * intra_cluster_costs - self.alpha)\n",
    "        denominator_mask = (classes.unsqueeze(1) != cluster_classes.unsqueeze(0)).float()\n",
    "        denominator = torch.sum(denominator_mask * torch.exp(var_normalizer * sample_costs), dim=1)\n",
    "\n",
    "        # Compute loss\n",
    "        epsilon = 1e-8\n",
    "        losses = torch.relu(-torch.log(numerator / (denominator + epsilon) + epsilon))\n",
    "        total_loss = torch.mean(losses)\n",
    "\n",
    "        return total_loss, losses.detach()\n",
    "\n",
    "class ClusterBatchBuilder: \n",
    "    def __init__(self, labels, k, m, d):\n",
    "        self.num_classes = len(np.unique(labels))\n",
    "        self.labels = labels\n",
    "        self.k = k\n",
    "        self.m = min(m, self.num_classes)\n",
    "        self.d = d\n",
    "        self.centroids = None\n",
    "        self.assignments = np.zeros_like(labels, int)\n",
    "        self.cluster_assignments = {}\n",
    "        self.cluster_classes = np.repeat(range(self.num_classes), k)\n",
    "        self.example_losses = None\n",
    "        self.cluster_losses = None\n",
    "\n",
    "    def update_clusters(self, rep_data, max_iter=20):\n",
    "        if self.centroids is None:\n",
    "            self.centroids = np.zeros([self.num_classes * self.k, rep_data.shape[1]])\n",
    "\n",
    "        for c in range(self.num_classes):\n",
    "            class_mask = self.labels == c\n",
    "            class_examples = rep_data[class_mask]\n",
    "            \n",
    "            if len(class_examples) == 0:\n",
    "                continue\n",
    "            \n",
    "            n_clusters = min(self.k, len(class_examples))\n",
    "            \n",
    "            if n_clusters == 1:\n",
    "                self.centroids[self.k * c] = class_examples[0]\n",
    "                self.assignments[class_mask] = self.k * c\n",
    "            else:\n",
    "                kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=1, max_iter=max_iter)\n",
    "                kmeans.fit(class_examples)\n",
    "\n",
    "                start = self.k * c\n",
    "                stop = start + n_clusters\n",
    "                self.centroids[start:stop] = kmeans.cluster_centers_\n",
    "                self.assignments[class_mask] = start + kmeans.labels_\n",
    "\n",
    "        self.cluster_assignments = {\n",
    "            cluster: np.where(self.assignments == cluster)[0]\n",
    "            for cluster in range(self.k * self.num_classes)\n",
    "            if np.any(self.assignments == cluster)\n",
    "        }\n",
    "\n",
    "    def update_losses(self, indexes, losses):\n",
    "        if self.example_losses is None:\n",
    "            self.example_losses = np.zeros_like(self.labels, float)\n",
    "            self.cluster_losses = np.zeros([self.k * self.num_classes], float)\n",
    "\n",
    "        self.example_losses[indexes] = losses\n",
    "        clusters = np.unique(self.assignments[indexes])\n",
    "        for cluster in clusters:\n",
    "            cluster_inds = self.assignments == cluster\n",
    "            self.cluster_losses[cluster] = np.mean(self.example_losses[cluster_inds])\n",
    "\n",
    "    def gen_batch(self):\n",
    "        if not self.cluster_assignments:\n",
    "            raise ValueError(\"No clusters available. Make sure update_clusters has been called with non-empty data.\")\n",
    "\n",
    "        available_clusters = list(self.cluster_assignments.keys())\n",
    "\n",
    "        clusters = []\n",
    "        batch_class_inds = []\n",
    "\n",
    "        while len(clusters) < self.m and len(available_clusters) > 0:\n",
    "            if self.cluster_losses is not None:\n",
    "                p = np.array([self.cluster_losses[c] for c in available_clusters])\n",
    "                if np.all(p == 0) or np.any(np.isnan(p)):\n",
    "                    next_cluster = np.random.choice(available_clusters)\n",
    "                else:\n",
    "                    p = p / np.sum(p)\n",
    "                    next_cluster = np.random.choice(available_clusters, p=p)\n",
    "            else:\n",
    "                next_cluster = np.random.choice(available_clusters)\n",
    "\n",
    "            if self.cluster_classes[next_cluster] not in batch_class_inds:\n",
    "                clusters.append(next_cluster)\n",
    "                batch_class_inds.extend([self.cluster_classes[next_cluster]] * self.d)\n",
    "\n",
    "            available_clusters.remove(next_cluster)\n",
    "\n",
    "        batch_indexes = []\n",
    "        for c in clusters:\n",
    "            cluster_examples = self.cluster_assignments[c]\n",
    "            if len(cluster_examples) < self.d:\n",
    "                x = np.random.choice(cluster_examples, self.d, replace=True)\n",
    "            else:\n",
    "                x = np.random.choice(cluster_examples, self.d, replace=False)\n",
    "            batch_indexes.extend(x)\n",
    "\n",
    "        return np.array(batch_indexes), np.array(batch_class_inds)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4627da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(299, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Divide the dataset into 5 sets, each containing 2 classes\n",
    "    class_indices = [np.where(np.array(trainset.targets) == i)[0] for i in range(10)]\n",
    "    test_class_indices = [np.where(np.array(testset.targets) == i)[0] for i in range(10)]\n",
    "\n",
    "    train_subsets = []\n",
    "    test_subsets = []\n",
    "    class_indices_per_increment = []\n",
    "    for i in range(0, 10, 2):\n",
    "        train_subset_indices = np.concatenate((class_indices[i], class_indices[i+1]))\n",
    "        test_subset_indices = np.concatenate((test_class_indices[i], test_class_indices[i+1]))\n",
    "        train_subsets.append(Subset(trainset, train_subset_indices))\n",
    "        test_subsets.append(Subset(testset, test_subset_indices))\n",
    "        class_indices_per_increment.append([i, i+1])\n",
    "\n",
    "    return train_subsets, test_subsets, class_indices_per_increment\n",
    "\n",
    "def train_increment(model, trainloader, optimizer, magnet_loss, k, m, d, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(tqdm(trainloader, desc=\"Training\")):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "        \n",
    "        loss = magnet_loss(outputs, labels, m, d)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Batch {i}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return running_loss / len(trainloader)\n",
    "\n",
    "# def evaluate(model, testloader, magnet_loss, m, d, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in tqdm(testloader, desc=\"Evaluating\"):\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "            \n",
    "#             if isinstance(outputs, tuple):\n",
    "#                 outputs = outputs[0]\n",
    "            \n",
    "#             # Adjust m and d based on the actual batch size\n",
    "#             batch_size = outputs.size(0)\n",
    "#             adjusted_m = max(2, min(m, batch_size // d))\n",
    "#             adjusted_d = batch_size // adjusted_m\n",
    "\n",
    "#             loss, _ = magnet_loss(outputs, labels, adjusted_m, adjusted_d)\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "            \n",
    "#             all_preds.extend(predicted.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#     accuracy = 100 * correct / total\n",
    "#     avg_loss = total_loss / len(testloader)\n",
    "#     unique_classes = np.unique(np.concatenate((all_preds, all_labels)))\n",
    "#     return accuracy, avg_loss, all_preds, all_labels, unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7439b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testloader, magnet_loss, m, d, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(testloader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = magnet_loss(outputs, labels, m, d)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(testloader)\n",
    "    unique_classes = np.unique(np.concatenate((all_preds, all_labels)))\n",
    "    return accuracy, avg_loss, all_preds, all_labels, unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5888d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visuals\n",
    "\n",
    "def plot_training_progress(train_losses, val_accuracies, save_path='training_progress.png'):\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Increments')\n",
    "    ax1.set_ylabel('Training Loss', color=color)\n",
    "    ax1.plot(train_losses, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Validation Accuracy', color=color)\n",
    "    ax2.plot(val_accuracies, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    plt.title('Training Progress')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def visualize_embeddings(model, dataloader, num_classes, save_path='embeddings_visualization.png'):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Computing embeddings\"):\n",
    "            inputs = inputs.to(next(model.parameters()).device)\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            embeddings.append(outputs.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    \n",
    "    embeddings = np.vstack(embeddings)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    print(\"Performing t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='tab10')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('t-SNE visualization of learned embeddings')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(all_preds, all_labels, class_names, save_path='confusion_matrix.png'):\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffc162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(num_classes=10):\n",
    "    \"\"\"Load pre-trained Inception-v3 model.\"\"\"\n",
    "    model = models.inception_v3(pretrained=False)\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    checkpoint = torch.load('pretrained_inception_v3_e2b100.pth')\n",
    "    model.load_state_dict(checkpoint)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6e724",
   "metadata": {},
   "source": [
    "Lower epochs due to GPU and time constraints thus increased the learning rate.\n",
    "\n",
    "1. Due to time constraints only ran till Increment 2 to see if the code is working. \n",
    "    Took ~30mins to get accuracy of 80% in the first increment.\n",
    "2. All the developed images are named with their respective increment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6333518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data and model loaded successfully.\n",
      "Increment 1/5\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                 | 0/40 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 346.00 MiB (GPU 0; 6.00 GiB total capacity; 14.81 GiB already allocated; 0 bytes free; 15.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContinual Learning completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 73\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[9], line 40\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_increment(model, trainloader, optimizer, magnet_loss, k, m, d, device)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Add gradient clipping\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m, in \u001b[0;36mtrain_increment\u001b[1;34m(model, trainloader, optimizer, magnet_loss, k, m, d, device)\u001b[0m\n\u001b[0;32m     34\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m     40\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\models\\inception.py:166\u001b[0m, in \u001b[0;36mInception3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InceptionOutputs:\n\u001b[0;32m    165\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_input(x)\n\u001b[1;32m--> 166\u001b[0m     x, aux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(x)\n\u001b[0;32m    167\u001b[0m     aux_defined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_logits\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\models\\inception.py:123\u001b[0m, in \u001b[0;36mInception3._forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    121\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMixed_5c(x)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# N x 288 x 35 x 35\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMixed_5d(x)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# N x 288 x 35 x 35\u001b[39;00m\n\u001b[0;32m    125\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMixed_6a(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\models\\inception.py:211\u001b[0m, in \u001b[0;36mInceptionA.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 211\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(x)\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\models\\inception.py:204\u001b[0m, in \u001b[0;36mInceptionA._forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    201\u001b[0m branch3x3dbl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch3x3dbl_2(branch3x3dbl)\n\u001b[0;32m    202\u001b[0m branch3x3dbl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch3x3dbl_3(branch3x3dbl)\n\u001b[1;32m--> 204\u001b[0m branch_pool \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mavg_pool2d(x, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    205\u001b[0m branch_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch_pool(branch_pool)\n\u001b[0;32m    207\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 346.00 MiB (GPU 0; 6.00 GiB total capacity; 14.81 GiB already allocated; 0 bytes free; 15.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 256\n",
    "    k = 10  # clusters per class\n",
    "    m = 4  # clusters per batch\n",
    "    d = 8  # examples per cluster\n",
    "    alpha = 1.0\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 2\n",
    "\n",
    "    # Load data\n",
    "    train_subsets, test_subsets, class_indices_per_increment = load_data(batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = load_model()\n",
    "    print(\"Data and model loaded successfully.\")\n",
    "    \n",
    "    # Initialize MagnetLoss and optimizer\n",
    "    magnet_loss = AdaptiveMagnetLoss(alpha).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Metrics storage\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Continual learning loop\n",
    "    for increment in range(5):\n",
    "        print(f\"Increment {increment + 1}/5\")\n",
    "\n",
    "        # Prepare data for current increment\n",
    "        trainloader = torch.utils.data.DataLoader(train_subsets[increment], batch_size=batch_size, shuffle=True)\n",
    "        testloader = torch.utils.data.DataLoader(ConcatDataset(test_subsets[:increment+1]), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Train for multiple epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            loss = train_increment(model, trainloader, optimizer, magnet_loss, k, m, d, device)\n",
    "            \n",
    "            # Add gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            print(f\"Training Loss: {loss:.4f}\")\n",
    "        \n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # Evaluate on all seen classes\n",
    "        accuracy, eval_loss, all_preds, all_labels, unique_classes = evaluate(model, testloader, magnet_loss, m, d, device)\n",
    "        val_accuracies.append(accuracy)\n",
    "        print(f\"Accuracy after increment {increment + 1}: {accuracy:.2f}%\")\n",
    "\n",
    "        # Save model after each increment\n",
    "        torch.save(model.state_dict(), f'model_increment_{increment+1}.pth')\n",
    "\n",
    "        # Visualizations\n",
    "        plot_training_progress(train_losses, val_accuracies, f'training_progress_increment_{increment+1}.png')\n",
    "        visualize_embeddings(model, testloader, len(unique_classes), f'embeddings_increment_{increment+1}.png')\n",
    "        \n",
    "        # Use the unique classes to determine the class names\n",
    "        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "        seen_class_names = [class_names[i] for i in range((increment+1)*2)]\n",
    "        plot_confusion_matrix(all_preds, all_labels, seen_class_names, f'confusion_matrix_increment_{increment+1}.png')\n",
    "\n",
    "        # Classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=seen_class_names, labels=range(len(seen_class_names))))\n",
    "\n",
    "    print(\"Continual Learning completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7006a4",
   "metadata": {},
   "source": [
    "As stated in the problem statement above is the model's accuracy on the test sets used till the current increment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe51c80",
   "metadata": {},
   "source": [
    "## IGNORE EXTRA CLIPS\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 64\n",
    "    k = 4  # clusters per class\n",
    "    m = 4  # clusters per batch\n",
    "    d = 8  # examples per cluster\n",
    "    alpha = 1.0\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 3  \n",
    "    # Load data\n",
    "    train_subsets, test_subsets, class_indices_per_increment = load_data(batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = torchvision.models.inception_v3(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)  # Keep 10 output classes\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize MagnetLoss and optimizer\n",
    "    magnet_loss = MagnetLoss(alpha).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Metrics storage\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "    # Continual learning loop\n",
    "    for increment in range(5):\n",
    "        print(f\"Increment {increment + 1}/5\")\n",
    "\n",
    "        # Get current class indices\n",
    "        current_class_indices = class_indices_per_increment[increment]\n",
    "        current_class_names = [class_names[i] for i in current_class_indices]\n",
    "\n",
    "        # Prepare data for current increment\n",
    "        trainloader = torch.utils.data.DataLoader(train_subsets[increment], batch_size=batch_size, shuffle=True)\n",
    "        testloader = torch.utils.data.DataLoader(ConcatDataset(test_subsets[:increment+1]), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Compute initial representations for current increment\n",
    "        model.eval()\n",
    "        initial_representations = []\n",
    "        initial_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(trainloader, desc=\"Computing initial representations\"):\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                initial_representations.append(outputs.cpu().numpy())\n",
    "                initial_labels.extend(labels.numpy())\n",
    "        initial_representations = np.vstack(initial_representations)\n",
    "        initial_labels = np.array(initial_labels)\n",
    "\n",
    "        # Initialize ClusterBatchBuilder for current increment\n",
    "        current_labels = np.array([trainset.targets[i] for i in train_subsets[increment].indices])\n",
    "        batch_builder = ClusterBatchBuilder(current_labels, k, m, d)\n",
    "        batch_builder.update_clusters(initial_representations)\n",
    "        \n",
    "        # Train for multiple epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            loss = train_increment(model, trainloader, optimizer, magnet_loss, batch_builder, k, m, d, alpha, device)\n",
    "            print(f\"Training Loss: {loss:.4f}\")\n",
    "        \n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # Evaluate on all seen classes\n",
    "        accuracy, all_preds, all_labels, unique_classes = evaluate(model, testloader, device)\n",
    "        val_accuracies.append(accuracy)\n",
    "        print(f\"Accuracy after increment {increment + 1}: {accuracy:.2f}%\")\n",
    "\n",
    "        # Save model after each increment\n",
    "        torch.save(model.state_dict(), f'model_increment_{increment+1}.pth')\n",
    "\n",
    "        # Visualizations\n",
    "        plot_training_progress(train_losses, val_accuracies, f'training_progress_increment_{increment+1}.png')\n",
    "        visualize_embeddings(model, testloader, len(unique_classes), f'embeddings_increment_{increment+1}.png')\n",
    "        \n",
    "        # Use the unique classes to determine the class names\n",
    "        seen_class_names = [class_names[i] for i in range((increment+1)*2)]\n",
    "        plot_confusion_matrix(all_preds, all_labels, seen_class_names, f'confusion_matrix_increment_{increment+1}.png')\n",
    "\n",
    "        # Classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=seen_class_names, labels=range(len(seen_class_names))))\n",
    "\n",
    "    print(\"Continual Learning completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168baa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
